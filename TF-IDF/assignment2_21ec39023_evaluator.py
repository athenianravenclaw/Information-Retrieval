# -*- coding: utf-8 -*-
"""Assignment2_21EC39023_evaluator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkG6WqSruu4ChHo7z93KC1s6z9aGT1s6
"""



import nltk
nltk.download('stopwords')

import nltk
nltk.download('wordnet')

import requests

url = 'https://ir.nist.gov/trec-covid/data/qrels-rnd1.txt'

# Send a request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Save the content to a local file
    with open('qrels-rnd1.txt', 'w') as file:
        file.write(response.text)
    print("File saved successfully.")
else:
    print("Failed to load the file.")

import requests

url = 'https://ir.nist.gov/trec-covid/data/topics-rnd1.xml'

# Send a request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Save the content to a local file
    with open('topics-rnd1.xml', 'w') as file:
        file.write(response.text)
    print("XML file saved successfully.")
else:
    print("Failed to load the XML file.")











import sys
import numpy as np

def read_relevance_file(relevance_file):
    relevance = {}
    with open('qrels-rnd1.txt', 'r') as f:
        for line in f:
            topic_id, iteration, cord_id, judgment = line.strip().split()
            topic_id = int(topic_id)
            if topic_id not in relevance:
                relevance[topic_id] = {}
            relevance[topic_id][cord_id] = int(judgment)
    return relevance

def read_ranked_list(ranked_list_file):
    ranked_list = {}
    with open('Assignment2_21EC39023_ranked_list_A.txt', 'r') as f:
        for line in f:
            topic_id, doc_ids = line.strip().split(':')
            topic_id = int(topic_id)
            ranked_list[topic_id] = doc_ids.split()
    return ranked_list

def average_precision(ranked_docs, relevance, k):
    relevant_count = 0
    total_precision = 0.0

    for i in range(min(k, len(ranked_docs))):
        doc_id = ranked_docs[i]
        if doc_id in relevance:  # Check using cord_id
            relevant_count += relevance[doc_id]  # Use the value directly
            total_precision += relevant_count / (i + 1)

    if relevant_count == 0:
        return 0.0

    return total_precision / min(k, len(ranked_docs))

def ndcg(ranked_docs, relevance, k):
    dcg = 0.0
    idcg = 0.0

    for i in range(min(k, len(ranked_docs))):
        doc_id = ranked_docs[i]
        rel = relevance.get(doc_id, 0)
        dcg += (2 ** rel - 1) / np.log2(i + 2)

    # Compute IDCG for top k documents
    ideal_relevances = sorted(relevance.values(), reverse=True)[:k]
    for i in range(len(ideal_relevances)):
        rel = ideal_relevances[i]
        idcg += (2 ** rel - 1) / np.log2(i + 2)

    if idcg == 0:
        return 0.0

    return dcg / idcg

def evaluate_metrics(relevance, ranked_list):
    results = {}
    for topic_id, ranked_docs in ranked_list.items():
        relevance_for_topic = relevance.get(topic_id, {})

        ap_10 = average_precision(ranked_docs, relevance_for_topic, 10)
        ap_20 = average_precision(ranked_docs, relevance_for_topic, 20)

        ndcg_10 = ndcg(ranked_docs, relevance_for_topic, 10)
        ndcg_20 = ndcg(ranked_docs, relevance_for_topic, 20)

        results[topic_id] = {
            'AP@10': ap_10,
            'AP@20': ap_20,
            'NDCG@10': ndcg_10,
            'NDCG@20': ndcg_20
        }

    return results

def calculate_average_metrics(metrics):
    mAP_10 = np.mean([v['AP@10'] for v in metrics.values()])
    mAP_20 = np.mean([v['AP@20'] for v in metrics.values()])
    averNDCG_10 = np.mean([v['NDCG@10'] for v in metrics.values()])
    averNDCG_20 = np.mean([v['NDCG@20'] for v in metrics.values()])

    return {
        'mAP@10': mAP_10,
        'mAP@20': mAP_20,
        'averNDCG@10': averNDCG_10,
        'averNDCG@20': averNDCG_20
    }

import sys
import numpy as np
from collections import defaultdict

# ... (keep your existing functions)

def write_metrics_to_file(metrics, averages, output_file):
    with open('Assignment2_21EC39023_metrics_A.txt', 'w') as f:
        for topic_id, values in metrics.items():
            f.write(f"Topic {topic_id}:\n")
            f.write(f"  AP@10: {values['AP@10']}\n")
            f.write(f"  AP@20: {values['AP@20']}\n")
            f.write(f"  NDCG@10: {values['NDCG@10']}\n")
            f.write(f"  NDCG@20: {values['NDCG@20']}\n")

        f.write("Averages:\n")
        f.write(f"  mAP@10: {averages['mAP@10']}\n")
        f.write(f"  mAP@20: {averages['mAP@20']}\n")
        f.write(f"  averNDCG@10: {averages['averNDCG@10']}\n")
        f.write(f"  averNDCG@20: {averages['averNDCG@20']}\n")

if __name__ == "__main__":
    gold_standard_file = sys.argv[1]
    ranked_list_file = sys.argv[2]

    relevance = read_relevance_file(gold_standard_file)
    ranked_list = read_ranked_list(ranked_list_file)

    metrics = evaluate_metrics(relevance, ranked_list)
    averages = calculate_average_metrics(metrics)

    output_file = f"Assignment2_21EC39023_metrics_A.txt"

    write_metrics_to_file(metrics, averages, output_file)

import sys
import numpy as np

def read_relevance_file(relevance_file):
    relevance = {}
    with open('qrels-rnd1.txt', 'r') as f:
        for line in f:
            topic_id, iteration, cord_id, judgment = line.strip().split()
            topic_id = int(topic_id)
            if topic_id not in relevance:
                relevance[topic_id] = {}
            relevance[topic_id][cord_id] = int(judgment)
    return relevance

def read_ranked_list(ranked_list_file):
    ranked_list = {}
    with open('Assignment2_21EC39023_ranked_list_B.txt', 'r') as f:
        for line in f:
            topic_id, doc_ids = line.strip().split(':')
            topic_id = int(topic_id)
            ranked_list[topic_id] = doc_ids.split()
    return ranked_list

def average_precision(ranked_docs, relevance, k):
    relevant_count = 0
    total_precision = 0.0

    for i in range(min(k, len(ranked_docs))):
        doc_id = ranked_docs[i]
        if doc_id in relevance:  # Check using cord_id
            relevant_count += relevance[doc_id]  # Use the value directly
            total_precision += relevant_count / (i + 1)

    if relevant_count == 0:
        return 0.0

    return total_precision / min(k, len(ranked_docs))

def ndcg(ranked_docs, relevance, k):
    dcg = 0.0
    idcg = 0.0

    for i in range(min(k, len(ranked_docs))):
        doc_id = ranked_docs[i]
        rel = relevance.get(doc_id, 0)
        dcg += (2 ** rel - 1) / np.log2(i + 2)

    # Compute IDCG for top k documents
    ideal_relevances = sorted(relevance.values(), reverse=True)[:k]
    for i in range(len(ideal_relevances)):
        rel = ideal_relevances[i]
        idcg += (2 ** rel - 1) / np.log2(i + 2)

    if idcg == 0:
        return 0.0

    return dcg / idcg

def evaluate_metrics(relevance, ranked_list):
    results = {}
    for topic_id, ranked_docs in ranked_list.items():
        relevance_for_topic = relevance.get(topic_id, {})

        ap_10 = average_precision(ranked_docs, relevance_for_topic, 10)
        ap_20 = average_precision(ranked_docs, relevance_for_topic, 20)

        ndcg_10 = ndcg(ranked_docs, relevance_for_topic, 10)
        ndcg_20 = ndcg(ranked_docs, relevance_for_topic, 20)

        results[topic_id] = {
            'AP@10': ap_10,
            'AP@20': ap_20,
            'NDCG@10': ndcg_10,
            'NDCG@20': ndcg_20
        }

    return results

def calculate_average_metrics(metrics):
    mAP_10 = np.mean([v['AP@10'] for v in metrics.values()])
    mAP_20 = np.mean([v['AP@20'] for v in metrics.values()])
    averNDCG_10 = np.mean([v['NDCG@10'] for v in metrics.values()])
    averNDCG_20 = np.mean([v['NDCG@20'] for v in metrics.values()])

    return {
        'mAP@10': mAP_10,
        'mAP@20': mAP_20,
        'averNDCG@10': averNDCG_10,
        'averNDCG@20': averNDCG_20
    }

import sys
import numpy as np
from collections import defaultdict

# ... (keep your existing functions)

def write_metrics_to_file(metrics, averages, output_file):
    with open('Assignment2_21EC39023_metrics_B.txt', 'w') as f:
        for topic_id, values in metrics.items():
            f.write(f"Topic {topic_id}:\n")
            f.write(f"  AP@10: {values['AP@10']}\n")
            f.write(f"  AP@20: {values['AP@20']}\n")
            f.write(f"  NDCG@10: {values['NDCG@10']}\n")
            f.write(f"  NDCG@20: {values['NDCG@20']}\n")

        f.write("Averages:\n")
        f.write(f"  mAP@10: {averages['mAP@10']}\n")
        f.write(f"  mAP@20: {averages['mAP@20']}\n")
        f.write(f"  averNDCG@10: {averages['averNDCG@10']}\n")
        f.write(f"  averNDCG@20: {averages['averNDCG@20']}\n")

if __name__ == "__main__":
    gold_standard_file = sys.argv[1]
    ranked_list_file = sys.argv[2]

    relevance = read_relevance_file(gold_standard_file)
    ranked_list = read_ranked_list(ranked_list_file)

    metrics = evaluate_metrics(relevance, ranked_list)
    averages = calculate_average_metrics(metrics)

    output_file = f"Assignment2_21EC39023_metrics_B.txt"

    write_metrics_to_file(metrics, averages, output_file)

import sys
import numpy as np

def read_relevance_file(relevance_file):
    relevance = {}
    with open('qrels-rnd1.txt', 'r') as f:
        for line in f:
            topic_id, iteration, cord_id, judgment = line.strip().split()
            topic_id = int(topic_id)
            if topic_id not in relevance:
                relevance[topic_id] = {}
            relevance[topic_id][cord_id] = int(judgment)
    return relevance

def read_ranked_list(ranked_list_file):
    ranked_list = {}
    with open('Assignment2_21EC39023_ranked_list_C.txt', 'r') as f:
        for line in f:
            topic_id, doc_ids = line.strip().split(':')
            topic_id = int(topic_id)
            ranked_list[topic_id] = doc_ids.split()
    return ranked_list

def average_precision(ranked_docs, relevance, k):
    relevant_count = 0
    total_precision = 0.0

    for i in range(min(k, len(ranked_docs))):
        doc_id = ranked_docs[i]
        if doc_id in relevance:  # Check using cord_id
            relevant_count += relevance[doc_id]  # Use the value directly
            total_precision += relevant_count / (i + 1)

    if relevant_count == 0:
        return 0.0

    return total_precision / min(k, len(ranked_docs))

def ndcg(ranked_docs, relevance, k):
    dcg = 0.0
    idcg = 0.0

    for i in range(min(k, len(ranked_docs))):
        doc_id = ranked_docs[i]
        rel = relevance.get(doc_id, 0)
        dcg += (2 ** rel - 1) / np.log2(i + 2)

    # Compute IDCG for top k documents
    ideal_relevances = sorted(relevance.values(), reverse=True)[:k]
    for i in range(len(ideal_relevances)):
        rel = ideal_relevances[i]
        idcg += (2 ** rel - 1) / np.log2(i + 2)

    if idcg == 0:
        return 0.0

    return dcg / idcg

def evaluate_metrics(relevance, ranked_list):
    results = {}
    for topic_id, ranked_docs in ranked_list.items():
        relevance_for_topic = relevance.get(topic_id, {})

        ap_10 = average_precision(ranked_docs, relevance_for_topic, 10)
        ap_20 = average_precision(ranked_docs, relevance_for_topic, 20)

        ndcg_10 = ndcg(ranked_docs, relevance_for_topic, 10)
        ndcg_20 = ndcg(ranked_docs, relevance_for_topic, 20)

        results[topic_id] = {
            'AP@10': ap_10,
            'AP@20': ap_20,
            'NDCG@10': ndcg_10,
            'NDCG@20': ndcg_20
        }

    return results

def calculate_average_metrics(metrics):
    mAP_10 = np.mean([v['AP@10'] for v in metrics.values()])
    mAP_20 = np.mean([v['AP@20'] for v in metrics.values()])
    averNDCG_10 = np.mean([v['NDCG@10'] for v in metrics.values()])
    averNDCG_20 = np.mean([v['NDCG@20'] for v in metrics.values()])

    return {
        'mAP@10': mAP_10,
        'mAP@20': mAP_20,
        'averNDCG@10': averNDCG_10,
        'averNDCG@20': averNDCG_20
    }

import sys
import numpy as np
from collections import defaultdict

# ... (keep your existing functions)

def write_metrics_to_file(metrics, averages, output_file):
    with open('Assignment2_21EC39023_metrics_C.txt', 'w') as f:
        for topic_id, values in metrics.items():
            f.write(f"Topic {topic_id}:\n")
            f.write(f"  AP@10: {values['AP@10']}\n")
            f.write(f"  AP@20: {values['AP@20']}\n")
            f.write(f"  NDCG@10: {values['NDCG@10']}\n")
            f.write(f"  NDCG@20: {values['NDCG@20']}\n")

        f.write("Averages:\n")
        f.write(f"  mAP@10: {averages['mAP@10']}\n")
        f.write(f"  mAP@20: {averages['mAP@20']}\n")
        f.write(f"  averNDCG@10: {averages['averNDCG@10']}\n")
        f.write(f"  averNDCG@20: {averages['averNDCG@20']}\n")

if __name__ == "__main__":
    gold_standard_file = sys.argv[1]
    ranked_list_file = sys.argv[2]

    relevance = read_relevance_file(gold_standard_file)
    ranked_list = read_ranked_list(ranked_list_file)

    metrics = evaluate_metrics(relevance, ranked_list)
    averages = calculate_average_metrics(metrics)

    output_file = f"Assignment2_21EC39023_metrics_C.txt"

    write_metrics_to_file(metrics, averages, output_file)