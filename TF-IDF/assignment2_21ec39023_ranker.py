# -*- coding: utf-8 -*-
"""Assignment2_21EC39023_ranker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_AocQQYAiiQPHM9sT2V7PR3NuqBJwh_
"""



import nltk
nltk.download('stopwords')

import nltk
nltk.download('wordnet')

import requests

url = 'https://ir.nist.gov/trec-covid/data/qrels-rnd1.txt'

# Send a request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Save the content to a local file
    with open('qrels-rnd1.txt', 'w') as file:
        file.write(response.text)
    print("File saved successfully.")
else:
    print("Failed to load the file.")

import requests

url = 'https://ir.nist.gov/trec-covid/data/topics-rnd1.xml'

# Send a request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Save the content to a local file
    with open('topics-rnd1.xml', 'w') as file:
        file.write(response.text)
    print("XML file saved successfully.")
else:
    print("Failed to load the XML file.")



import pandas as pd
data = pd.read_csv('/content/sampled_data.csv')

data.head()

data.dropna(subset=['abstract'], inplace=True)

import os
import re
import math
import pickle
import numpy as np
from collections import defaultdict
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import xml.etree.ElementTree as ET

# Preprocessing function
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = [word for word in text.lower().split() if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

# Build the inverted index from the XML file
def build_inverted_index(data_path):
    inverted_index = defaultdict(list)
    df = pd.read_csv(os.path.join(data_path, '/content/sampled_data.csv'))  # Replace with your actual CSV file name
    df.dropna(subset=['abstract'], inplace=True)

    for _, row in df.iterrows():
        doc_id = row['cord_uid']  # Assuming 'cord_uid' is the unique identifier column
        abstract = row['abstract']  # Assuming 'abstract' is the column with text
        tokens = preprocess_text(abstract)
        for token in tokens:
            inverted_index[token].append(doc_id)

    with open('model_queries_21EC39023.bin', 'wb') as f:
        pickle.dump(inverted_index, f)

# Calculate TF-IDF weights
def compute_tfidf_weights(inverted_index, num_docs):
    df = {term: len(postings) for term, postings in inverted_index.items()}
    idf = {term: math.log(num_docs / df_val) for term, df_val in df.items()}
    return idf

# Create document vectors with different weighting schemes
from collections import defaultdict

def create_document_vectors(inverted_index, idf, num_docs, scheme):
    term_index = {term: idx for idx, term in enumerate(inverted_index.keys())}
    num_terms = len(term_index)
    doc_vectors = defaultdict(lambda: np.zeros(num_terms))

    for term, postings in inverted_index.items():
        # Count term frequencies in one pass
        tf_counts = defaultdict(int)
        for doc_id in postings:
            tf_counts[doc_id] += 1

        # Find the maximum term frequency in the current postings
        max_tf = max(tf_counts.values()) if tf_counts else 0

        for doc_id, tf in tf_counts.items():
            if scheme == 'lnc.ltc':  # Natural TF
                weight = tf * (idf[term] if term in idf else 0)

            elif scheme == 'lnc.Ltc':  # Logarithmic TF
                weight = (1 + np.log(tf)) * (idf[term] if term in idf else 0)

            elif scheme == 'anc.apc':  # Augmented TF
                weight = (0.5 + 0.5 * (tf / max_tf)) * (idf[term] if term in idf else 0) if max_tf > 0 else 0

            if term in term_index:
                doc_vectors[doc_id][term_index[term]] = weight

    return doc_vectors

# Cosine similarity
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# Ranking documents for queries
def rank_documents(query_vector, doc_vectors):
    rankings = {}
    for doc_id, doc_vector in doc_vectors.items():
        sim = cosine_similarity(query_vector, doc_vector)
        rankings[doc_id] = sim
    return sorted(rankings.items(), key=lambda x: x[1], reverse=True)[:50]

# Main function to rank the documents for each query
def rank_documents_for_queries(data_path):
    with open('model_queries_21EC39023.bin', 'rb') as f:
        inverted_index = pickle.load(f)

    num_docs = len(inverted_index)
    idf = compute_tfidf_weights(inverted_index, num_docs)

    for scheme in ['lnc.ltc', 'lnc.Ltc', 'anc.apc']:
        doc_vectors = create_document_vectors(inverted_index, idf, num_docs, scheme)

        # Create a term index for query processing
        term_index = {term: idx for idx, term in enumerate(inverted_index.keys())}

        # Process queries
        queries = ET.parse(os.path.join(data_path, '/content/topics-rnd1.xml')).getroot()
        for topic in queries.findall('topic'):
            query_text = topic.find('query').text
            query_tokens = preprocess_text(query_text)
            query_vector = np.zeros(len(inverted_index))

            for token in query_tokens:
                if token in idf:
                    query_vector[term_index[token]] = idf[token]

            ranked_docs = rank_documents(query_vector, doc_vectors)
            if scheme == 'lnc.ltc':
                output_file = 'Assignment2_21EC39023_ranked_list_A.txt'
            elif scheme == 'lnc.Ltc':
                output_file = 'Assignment2_21EC39023_ranked_list_B.txt'
            elif scheme == 'anc.apc':
                output_file = 'Assignment2_21EC39023_ranked_list_C.txt'

            with open(output_file, 'a') as f:
                f.write(f"{topic.get('number')}: {' '.join([doc[0] for doc in ranked_docs])}\n")

if __name__ == "__main__":
    import sys
    data_path = sys.argv[1]
    build_inverted_index(data_path)  # Build the inverted index
    rank_documents_for_queries(data_path)  # Rank documents for queries



